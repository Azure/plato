{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Plato Toolkit documentation Learn how to train and deploy RL agents at scale with Ray RLlib and Azure Machine Learning (AML). Overview Prerequisites Create Azure Resources AML Environment Setup Custom Simulation Environment Samples Simple Adder : A minimal working example of a Python simulator that can be connected to RLlib and used to train an agent on AML. You can think of it as a \"Hello World\" sample. User Guides Create Azure Resources To use this toolkit, you'll need the following: An Azure subscription and a resource group An AML workspace An AML compute cluster You can also create these using the AML Python SDK . Selecting a Compute Cluster Size There is no definitive answer to how to select a compute cluster size for RL, as it depends on many factors (e.g., your project budget, simulation environment, and model architecture). However, some general guidelines are: Unless you have a compute intensive RL model (e.g., a large deep residual network), we recommend selecting a general purpose CPU VM. Choose a minimum number of nodes that defines how many nodes are always running and ready for your jobs. We recommend selecting 0 as your minimum to de-allocate the nodes when they aren't in use. Any value larger than 0 will keep that number of nodes running and incur cost. Choose a maximum number of nodes that defines how many nodes can be added to scale up your training when needed. Avoid large unexpected Azure costs by familiarizing yourself with the size and cost of Azure VMs . If you are still unsure which VM to select, a cluster with 6 CPU cores and 64GB RAM should be a good starting point for most RL workloads using a Python simulation environment. You can also monitor your job's resource utilization in AML studio during experiment runs and adjust your VM size accordingly. Once you have an AML workspace that contains a compute cluster ready to go, the next step is to set up an AML environment to add your Python package dependencies. AML Environment Setup A user-managed AML environment specifies the Python packages required to run your simulation and Ray RLlib code. You can follow the how-to guide on configuring AML environments or try our preferred method below using a conda file. We've provided a conda.yml file and Azure CLI command that you can use to create an environment for the Simple Adder sample within this toolkit. Simply save the file and run the CLI command from the same location. For more detailed instructions, you can follow the guide to create an environment from a conda file in AML studio or with the AML Python SDK . # conda.yml channels: - anaconda - conda-forge dependencies: - python=3.8.5 - pip=22.3.1 - pip: # Dependencies for Ray on AML - azureml-mlflow - azureml-defaults - ray-on-aml - ray[data]==2.3.0 - ray[rllib]==2.3.0 # Dependencies for RLlib - tensorflow==2.11.1 # Dependencies for the Simulator - gymnasium - numpy==1.24.2 Azure CLI command: Azure CLI az ml environment create --name aml-environment --conda-file conda.yml --image mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04 --resource-group $YOUR_RESOURCE_GROUP --workspace-name $YOUR_WORKSPACE Custom Simulation Environment with Gymnasium Before you can train an RL agent on AML, your simulation environment needs to be compatible with Ray RLlib. For Python simulation environments, we recommend modifying your code to create a custom Gymnasium environment by following this tutorial and using the samples in this repository for reference. The basic steps are: Implement the gymnasium.Env interface and define methods for reset() and step() . Specify the action_space and observation_space attributes during initialization using gymnasium.spaces . Ensure that the actions and observations returned by reset() and step() have the same shape and dtype as the action_space and observation_space defined. For example, if your observation_space is a gymnasium.spaces.Box space with shape=(1,) and dtype=np.float32, you should make sure that your observation is a numpy array of shape (1,) and dtype np.float32. After you complete the integration, we suggest that you confirm it can run on your local machine before scaling on AML. Our Simple Adder sample provides you with the code to run it both locally and on AML.","title":"Home"},{"location":"#plato-toolkit-documentation","text":"Learn how to train and deploy RL agents at scale with Ray RLlib and Azure Machine Learning (AML).","title":"Plato Toolkit documentation"},{"location":"#overview","text":"Prerequisites Create Azure Resources AML Environment Setup Custom Simulation Environment Samples Simple Adder : A minimal working example of a Python simulator that can be connected to RLlib and used to train an agent on AML. You can think of it as a \"Hello World\" sample. User Guides","title":"Overview"},{"location":"#create-azure-resources","text":"To use this toolkit, you'll need the following: An Azure subscription and a resource group An AML workspace An AML compute cluster You can also create these using the AML Python SDK .","title":"Create Azure Resources"},{"location":"#selecting-a-compute-cluster-size","text":"There is no definitive answer to how to select a compute cluster size for RL, as it depends on many factors (e.g., your project budget, simulation environment, and model architecture). However, some general guidelines are: Unless you have a compute intensive RL model (e.g., a large deep residual network), we recommend selecting a general purpose CPU VM. Choose a minimum number of nodes that defines how many nodes are always running and ready for your jobs. We recommend selecting 0 as your minimum to de-allocate the nodes when they aren't in use. Any value larger than 0 will keep that number of nodes running and incur cost. Choose a maximum number of nodes that defines how many nodes can be added to scale up your training when needed. Avoid large unexpected Azure costs by familiarizing yourself with the size and cost of Azure VMs . If you are still unsure which VM to select, a cluster with 6 CPU cores and 64GB RAM should be a good starting point for most RL workloads using a Python simulation environment. You can also monitor your job's resource utilization in AML studio during experiment runs and adjust your VM size accordingly. Once you have an AML workspace that contains a compute cluster ready to go, the next step is to set up an AML environment to add your Python package dependencies.","title":"Selecting a Compute Cluster Size"},{"location":"#aml-environment-setup","text":"A user-managed AML environment specifies the Python packages required to run your simulation and Ray RLlib code. You can follow the how-to guide on configuring AML environments or try our preferred method below using a conda file. We've provided a conda.yml file and Azure CLI command that you can use to create an environment for the Simple Adder sample within this toolkit. Simply save the file and run the CLI command from the same location. For more detailed instructions, you can follow the guide to create an environment from a conda file in AML studio or with the AML Python SDK . # conda.yml channels: - anaconda - conda-forge dependencies: - python=3.8.5 - pip=22.3.1 - pip: # Dependencies for Ray on AML - azureml-mlflow - azureml-defaults - ray-on-aml - ray[data]==2.3.0 - ray[rllib]==2.3.0 # Dependencies for RLlib - tensorflow==2.11.1 # Dependencies for the Simulator - gymnasium - numpy==1.24.2 Azure CLI command: Azure CLI az ml environment create --name aml-environment --conda-file conda.yml --image mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04 --resource-group $YOUR_RESOURCE_GROUP --workspace-name $YOUR_WORKSPACE","title":"AML Environment Setup"},{"location":"#custom-simulation-environment-with-gymnasium","text":"Before you can train an RL agent on AML, your simulation environment needs to be compatible with Ray RLlib. For Python simulation environments, we recommend modifying your code to create a custom Gymnasium environment by following this tutorial and using the samples in this repository for reference. The basic steps are: Implement the gymnasium.Env interface and define methods for reset() and step() . Specify the action_space and observation_space attributes during initialization using gymnasium.spaces . Ensure that the actions and observations returned by reset() and step() have the same shape and dtype as the action_space and observation_space defined. For example, if your observation_space is a gymnasium.spaces.Box space with shape=(1,) and dtype=np.float32, you should make sure that your observation is a numpy array of shape (1,) and dtype np.float32. After you complete the integration, we suggest that you confirm it can run on your local machine before scaling on AML. Our Simple Adder sample provides you with the code to run it both locally and on AML.","title":"Custom Simulation Environment with Gymnasium"},{"location":"glossary/","text":"Glossary RLlib Terms Action : A decision made by the agent to change the state of the environment. Algorithm : A set of instructions that an agent follows to learn how to behave in an environment by performing actions and receiving feedback (reward/penalty) based on those actions. Agent : The learner and decision maker that interacts with an environment and receives a reward signal based on its actions. Batch : A collection of steps that are used to update a policy. Environment (Simulation) : A simulation of a real-world scenario that an agent interacts with. Episode : A sequence of actions taken by an agent from an initial state to either a \u201csuccess\u201d or \u201cfailure\u201d causing the environment to reach its \u201cterminal\u201d state. At each step, the agent receives an observation (i.e., the observable states of the environment), takes an action, and receives a reward. Iteration : A single training call for an RLlib Trainer (calling Trainer.train() once). An iteration may contain one or more episodes (collecting data for the train batch or for a replay buffer), and one or more SGD update steps, depending on the particular Trainer being used. NOTE: In RLlib, iteration should not be confused with the term step . Gymnasium : An open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of example environments compliant with the API. Observation : The part of a state that the agent can observe. Policy : A function mapping of the environment\u2019s observational states to an action to take, usually written \u03c0 (s(t)) -> a(t). Ray : A distributed computing framework that makes it easy to scale your applications and to leverage state-of-the-art machine learning libraries such as RLlib. Reward : A scalar value that indicates how well the agent is doing at a given step. For each good action, the agent gets positive feedback/reward, and for each bad action, the agent gets negative feedback/reward or penalty. RLlib : An open source Python library that provides scalable and easy-to-use reinforcement learning solutions. Rollout worker (Ray Actor) : A process that interacts with an environment and collects trajectories for training. State : A set of information that an agent has about the environment at a given time. States should have the Markov property, which means that knowing the state means you know everything that could determine the response of the environment to a specific action. Step : A single interaction between an agent and an environment, which consists of an observation (i.e., the state of the environment), an action, a reward, and a new observation. Azure Machine Learning Terms Compute cluster : A managed-compute infrastructure that allows you to easily create a single or multi-node resource for training or inference. Environment (AML) : A collection of software dependencies and configurations that are needed to run your reinforcement learning code on AML. Workspace : A top-level resource for your machine learning activities, providing a centralized place to view and manage the artifacts you create when you use AML. A workspace contains your experiments, models, datastores, compute targets, environments, and other resources.","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#rllib-terms","text":"Action : A decision made by the agent to change the state of the environment. Algorithm : A set of instructions that an agent follows to learn how to behave in an environment by performing actions and receiving feedback (reward/penalty) based on those actions. Agent : The learner and decision maker that interacts with an environment and receives a reward signal based on its actions. Batch : A collection of steps that are used to update a policy. Environment (Simulation) : A simulation of a real-world scenario that an agent interacts with. Episode : A sequence of actions taken by an agent from an initial state to either a \u201csuccess\u201d or \u201cfailure\u201d causing the environment to reach its \u201cterminal\u201d state. At each step, the agent receives an observation (i.e., the observable states of the environment), takes an action, and receives a reward. Iteration : A single training call for an RLlib Trainer (calling Trainer.train() once). An iteration may contain one or more episodes (collecting data for the train batch or for a replay buffer), and one or more SGD update steps, depending on the particular Trainer being used. NOTE: In RLlib, iteration should not be confused with the term step . Gymnasium : An open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of example environments compliant with the API. Observation : The part of a state that the agent can observe. Policy : A function mapping of the environment\u2019s observational states to an action to take, usually written \u03c0 (s(t)) -> a(t). Ray : A distributed computing framework that makes it easy to scale your applications and to leverage state-of-the-art machine learning libraries such as RLlib. Reward : A scalar value that indicates how well the agent is doing at a given step. For each good action, the agent gets positive feedback/reward, and for each bad action, the agent gets negative feedback/reward or penalty. RLlib : An open source Python library that provides scalable and easy-to-use reinforcement learning solutions. Rollout worker (Ray Actor) : A process that interacts with an environment and collects trajectories for training. State : A set of information that an agent has about the environment at a given time. States should have the Markov property, which means that knowing the state means you know everything that could determine the response of the environment to a specific action. Step : A single interaction between an agent and an environment, which consists of an observation (i.e., the state of the environment), an action, a reward, and a new observation.","title":"RLlib Terms"},{"location":"glossary/#azure-machine-learning-terms","text":"Compute cluster : A managed-compute infrastructure that allows you to easily create a single or multi-node resource for training or inference. Environment (AML) : A collection of software dependencies and configurations that are needed to run your reinforcement learning code on AML. Workspace : A top-level resource for your machine learning activities, providing a centralized place to view and manage the artifacts you create when you use AML. A workspace contains your experiments, models, datastores, compute targets, environments, and other resources.","title":"Azure Machine Learning Terms"},{"location":"user_guides/hyperparameter-tuning/","text":"Hyperparameter Tuning with Ray Tune Overview Reinforcement learning algorithms are notoriously tricky to optimize and train efficiently. Hyperparameter tuning with an efficient scheduler can often help you find a better performing agent in less time than a manual search. The example in examples/hyperparameter-tuning-and-monitoring demonstrates an example of how to use Population Based Training (PBT) algorithm to tune hyperparameters using Ray Tune library with MLflow on AzureML or locally. There are a few design decisions you as a user will be able to make when using this example, and this user guide will help you understand how to make those decisions. Why Population Based Training? Ray Tune provides a number of schedulers that can be used to tune hyperparameters for reinforcement learning algorithms. The scheduler we use in this sample, the Population Based Training (PBT) algorithm, is one of the most popular schedulers for reinforcement learning algorithms. While there are other schedulers in Ray that you can leverage for hyperparameter search, we recommend PBT for a few reasons: Efficient Exploration : PBT is based on the idea of iteratively exploring and exploiting the hyperparameter search space. This approach enables PBT to quickly explore a wide range of hyperparameters and identify promising ones for further optimization. As a result, PBT can often converge to good hyperparameter settings faster than other methods. Dynamic Adaptation : PBT can adapt hyperparameters dynamically during the training process. This means that PBT can change the hyperparameters as the training progresses and adapt to changing conditions. For example, if a particular set of hyperparameters is performing well, PBT can allocate more resources to it, while reducing the resources allocated to poorly performing hyperparameters. Resource Efficiency : PBT is resource-efficient because it optimizes hyperparameters in a distributed manner. Instead of training a single model with a fixed set of hyperparameters, PBT trains a population of models with different hyperparameters concurrently. This approach makes PBT more efficient because it can explore the search space faster and avoid getting stuck in local minima. A good overview of population based training is available here . How to Select Hyperparameter Mutations Space? The first design decision you will need to make is the hyperparameter_mutations dictionary. This dictionary specifies the mutations of the hyperparameters to be tuned by the PBT algorithm. The mutations are specified as a function of the search space for each hyperparameter. For example, the following code snippet shows the default hyperparameter mutations for the PPO algorithm in our sample: hyperparam_mutations = { \"lambda\": lambda: random.uniform(0.9, 1.0), \"clip_param\": lambda: random.uniform(0.01, 0.5), \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5], \"num_sgd_iter\": lambda: random.randint(1, 30), \"sgd_minibatch_size\": lambda: random.randint(128, 16384), \"train_batch_size\": lambda: random.randint(2000, 160000), } Here hyperparam_mutations is a dictionary where key/value pair specifies a resampling function for each hyperparameter, or a range of values to sample from. For example, the lambda hyperparameter is resampled from a uniform distribution between 0.9 and 1.0 whereas the lr hyperparameter is resampled from a list of allowed values. This dictionary is then passed to the PopulationBasedTraining scheduler as follows: pbt = PopulationBasedTraining( time_attr=\"time_total_s\", perturbation_interval=120, resample_probability=0.25, hyperparam_mutations=hyperparam_mutations, custom_explore_fn=explore, ) where we additionally specify the attribute to use for time tracking and comparison ( time_attr ), the frequency (as a multiple of time_attr , so 120 seconds in this example) at which to continue the trial (exploit) or perturb (explore) the hyperparameters ( perturbation_interval ), the probability of resampling a hyperparameter (vs mutating) from their resampling distribution ( resample_probability ), and a custom function to explore the hyperparameter space ( custom_explore_fn ), which in our example is set to simply ensure that sufficient environment samples have been collected for a training iteration. How to Select the Stopping Criteria? The next design decision you will need to make is the when to stop your experiment, which is specified through the stopping_criteria dictionary. This dictionary specifies the stopping criteria for the PBT algorithm. The stopping criteria are specified as a function of the training iteration and the mean episode reward of the model. For example, the following code snippet shows the default stopping criteria for the PPO algorithm in our sample: stopping_criteria = {\"training_iteration\": 100, \"episode_reward_mean\": 300} which specifies that the algorithm should stop when either the number of training iterations reaches 100 or the mean episode reward (summed over episodes and averaged over a training batch, i.e., \\(\\frac{1}{N} \\sum_{j=1}^{N}\\sum_{i=1}^T r_{i,j}\\) , where there are \\(N\\) episodes in the training batch and \\(T\\) iterations in each episode) reaches 300. You should set this based on what your desired average reward is (and be aware of the episode horizon) and how long you want to train for. Note that both of these are in terms of training iterations (where each training iteration is one SGD update), which would include a different number of environment steps because we are modifying the sgd_minibatch_size in our hyperparameters. Further guidance on defining stopping criteria for Tune expeirments is available in the Ray docs . Instantiating the Tuner The Tuner object is created using the Tuner class from Ray Tune. The TuneConfig parameter of the Tuner constructor specifies the tuning configuration, including the metric used for evaluation, the mode of optimization (maximization or minimization), the scheduler object that specifies the PBT algorithm and the number of samples to be evaluated by the PBT algorithm. This is finally fed into the Tuner itself, along with the algorithm to be tuned, the initial configuration of the algorithm, and the number of workers to use for sampling: tuner = tune.Tuner( # the algorithm/trainable to be tuned \"PPO\", tune_config=tune.TuneConfig( metric=\"episode_reward_mean\", mode=\"max\", scheduler=pbt, # the number of hyperparameters to sample num_samples=num_tune_samples, ), # specify the initial config input into the trainer # these are the initial samples used, which are then mutated by # the population based training algorithm if they are specified in # `hyperparam_mutations`. # the `num_workers` specifies the number of sample collection workers # that are used for gathering samples # the `num_cpus` specifies the number of CPUs for each training trial # here `num_workers=4` and `num_cpus=1` means we will use 5 cpus # if you want to run these trials concurrently, then you will need # CLUSTER_CPUS >= 5 x num_tune_samples # otherwise the PBT scheduler will round-robin between training each trial param_space={ \"env\": env_name, \"kl_coeff\": 1.0, \"num_workers\": 4, \"num_cpus\": 1, # number of CPUs to use per trial \"num_gpus\": 0, # number of GPUs to use per trial # For DiagGaussian action distributions, make the second half of the model # outputs floating bias variables instead of state-dependent. This only # has an effect is using the default fully connected net. # avoid this for non-continuous action spaces \"model\": {\"free_log_std\": True}, # These params are tuned from a fixed starting value. \"lambda\": 0.95, \"clip_param\": 0.2, \"lr\": 1e-4, # These params start off randomly drawn from a set. \"num_sgd_iter\": tune.choice([10, 20, 30]), \"sgd_minibatch_size\": tune.choice([128, 512, 2048]), \"train_batch_size\": tune.choice([10000, 20000, 40000]), }, # MLFlow callback uses parent_run_id and tracks all hyperparameter # runs as child jobs run_config=air.RunConfig( stop=stopping_criteria, callbacks=[ MLflowLoggerCallback( tags={MLFLOW_PARENT_RUN_ID: current_run.info.run_id}, experiment_name=\"pbt_ppo\", save_artifact=True, ) ], ), ) Here we are passing the algorithm we want to tune (PPO), the tuning configuration, the initial configuration of the algorithm, and the number of workers to use for sampling. Param Space vs Hyperparameter Mutations In the above Tuner call, we pass the param_space dictionary, which specifies the initial configuration of the algorithm. This is the configuration that is used to initialize the algorithm and is then mutated by the PBT algorithm. If the parameters were ommitted from param_space but were provided in hyperparam_mutations , then the initial values would be also sampled from hyperparam_mutations . The hyperparameters that are specified in the param_space dictionary are not mutated by the PBT algorithm unless they are also in hyperparam_mutations . For example, in the above code snippet, the num_workers hyperparameter is specified in the param_space dictionary but not in hyperparam_mutations , and is therefore not mutated by the PBT algorithm. Conversely, the num_sgd_iter hyperparameter is specified in the hyperparam_mutations dictionary and is mutated by the PBT algorithm. Selecting the Number of CPUs and Workers for Tuning Since PBT can train multiple trials concurrently, you can speed up the trial runtime by leveraging available CPUs to parallelize training. To do so effectively, you will need to specify the number of CPUs and workers to use for tuning. The num_cpus parameter specifies the number of CPUs to use for each trial, and the num_workers parameter specifies the number of workers to use for sampling from your simulation environments. In the above code snippet, we are using 4 workers and 1 CPU per trial, so we will need 5 CPUs to run each trial. If you do not have enough CPUs to run all the trials concurrently (i.e., 5 * num_tune_samples ), then the PBT scheduler will round-robin between training each subsequent trial. For example, if you only have 10 CPUs (as we have requested in our AML job.yml for this experiment), then the PBT scheduler will train 2 trials concurrently, and then train the remaining trials sequentially as resources become available. Further Resources Using RLlib with Tune Visualizing PBT Guide to Parallelism and Resources for Ray Tune A Guide to PBT with Tune","title":"Hyperparameter Tuning"},{"location":"user_guides/hyperparameter-tuning/#hyperparameter-tuning-with-ray-tune","text":"","title":"Hyperparameter Tuning with Ray Tune"},{"location":"user_guides/hyperparameter-tuning/#overview","text":"Reinforcement learning algorithms are notoriously tricky to optimize and train efficiently. Hyperparameter tuning with an efficient scheduler can often help you find a better performing agent in less time than a manual search. The example in examples/hyperparameter-tuning-and-monitoring demonstrates an example of how to use Population Based Training (PBT) algorithm to tune hyperparameters using Ray Tune library with MLflow on AzureML or locally. There are a few design decisions you as a user will be able to make when using this example, and this user guide will help you understand how to make those decisions.","title":"Overview"},{"location":"user_guides/hyperparameter-tuning/#why-population-based-training","text":"Ray Tune provides a number of schedulers that can be used to tune hyperparameters for reinforcement learning algorithms. The scheduler we use in this sample, the Population Based Training (PBT) algorithm, is one of the most popular schedulers for reinforcement learning algorithms. While there are other schedulers in Ray that you can leverage for hyperparameter search, we recommend PBT for a few reasons: Efficient Exploration : PBT is based on the idea of iteratively exploring and exploiting the hyperparameter search space. This approach enables PBT to quickly explore a wide range of hyperparameters and identify promising ones for further optimization. As a result, PBT can often converge to good hyperparameter settings faster than other methods. Dynamic Adaptation : PBT can adapt hyperparameters dynamically during the training process. This means that PBT can change the hyperparameters as the training progresses and adapt to changing conditions. For example, if a particular set of hyperparameters is performing well, PBT can allocate more resources to it, while reducing the resources allocated to poorly performing hyperparameters. Resource Efficiency : PBT is resource-efficient because it optimizes hyperparameters in a distributed manner. Instead of training a single model with a fixed set of hyperparameters, PBT trains a population of models with different hyperparameters concurrently. This approach makes PBT more efficient because it can explore the search space faster and avoid getting stuck in local minima. A good overview of population based training is available here .","title":"Why Population Based Training?"},{"location":"user_guides/hyperparameter-tuning/#how-to-select-hyperparameter-mutations-space","text":"The first design decision you will need to make is the hyperparameter_mutations dictionary. This dictionary specifies the mutations of the hyperparameters to be tuned by the PBT algorithm. The mutations are specified as a function of the search space for each hyperparameter. For example, the following code snippet shows the default hyperparameter mutations for the PPO algorithm in our sample: hyperparam_mutations = { \"lambda\": lambda: random.uniform(0.9, 1.0), \"clip_param\": lambda: random.uniform(0.01, 0.5), \"lr\": [1e-3, 5e-4, 1e-4, 5e-5, 1e-5], \"num_sgd_iter\": lambda: random.randint(1, 30), \"sgd_minibatch_size\": lambda: random.randint(128, 16384), \"train_batch_size\": lambda: random.randint(2000, 160000), } Here hyperparam_mutations is a dictionary where key/value pair specifies a resampling function for each hyperparameter, or a range of values to sample from. For example, the lambda hyperparameter is resampled from a uniform distribution between 0.9 and 1.0 whereas the lr hyperparameter is resampled from a list of allowed values. This dictionary is then passed to the PopulationBasedTraining scheduler as follows: pbt = PopulationBasedTraining( time_attr=\"time_total_s\", perturbation_interval=120, resample_probability=0.25, hyperparam_mutations=hyperparam_mutations, custom_explore_fn=explore, ) where we additionally specify the attribute to use for time tracking and comparison ( time_attr ), the frequency (as a multiple of time_attr , so 120 seconds in this example) at which to continue the trial (exploit) or perturb (explore) the hyperparameters ( perturbation_interval ), the probability of resampling a hyperparameter (vs mutating) from their resampling distribution ( resample_probability ), and a custom function to explore the hyperparameter space ( custom_explore_fn ), which in our example is set to simply ensure that sufficient environment samples have been collected for a training iteration.","title":"How to Select Hyperparameter Mutations Space?"},{"location":"user_guides/hyperparameter-tuning/#how-to-select-the-stopping-criteria","text":"The next design decision you will need to make is the when to stop your experiment, which is specified through the stopping_criteria dictionary. This dictionary specifies the stopping criteria for the PBT algorithm. The stopping criteria are specified as a function of the training iteration and the mean episode reward of the model. For example, the following code snippet shows the default stopping criteria for the PPO algorithm in our sample: stopping_criteria = {\"training_iteration\": 100, \"episode_reward_mean\": 300} which specifies that the algorithm should stop when either the number of training iterations reaches 100 or the mean episode reward (summed over episodes and averaged over a training batch, i.e., \\(\\frac{1}{N} \\sum_{j=1}^{N}\\sum_{i=1}^T r_{i,j}\\) , where there are \\(N\\) episodes in the training batch and \\(T\\) iterations in each episode) reaches 300. You should set this based on what your desired average reward is (and be aware of the episode horizon) and how long you want to train for. Note that both of these are in terms of training iterations (where each training iteration is one SGD update), which would include a different number of environment steps because we are modifying the sgd_minibatch_size in our hyperparameters. Further guidance on defining stopping criteria for Tune expeirments is available in the Ray docs .","title":"How to Select the Stopping Criteria?"},{"location":"user_guides/hyperparameter-tuning/#instantiating-the-tuner","text":"The Tuner object is created using the Tuner class from Ray Tune. The TuneConfig parameter of the Tuner constructor specifies the tuning configuration, including the metric used for evaluation, the mode of optimization (maximization or minimization), the scheduler object that specifies the PBT algorithm and the number of samples to be evaluated by the PBT algorithm. This is finally fed into the Tuner itself, along with the algorithm to be tuned, the initial configuration of the algorithm, and the number of workers to use for sampling: tuner = tune.Tuner( # the algorithm/trainable to be tuned \"PPO\", tune_config=tune.TuneConfig( metric=\"episode_reward_mean\", mode=\"max\", scheduler=pbt, # the number of hyperparameters to sample num_samples=num_tune_samples, ), # specify the initial config input into the trainer # these are the initial samples used, which are then mutated by # the population based training algorithm if they are specified in # `hyperparam_mutations`. # the `num_workers` specifies the number of sample collection workers # that are used for gathering samples # the `num_cpus` specifies the number of CPUs for each training trial # here `num_workers=4` and `num_cpus=1` means we will use 5 cpus # if you want to run these trials concurrently, then you will need # CLUSTER_CPUS >= 5 x num_tune_samples # otherwise the PBT scheduler will round-robin between training each trial param_space={ \"env\": env_name, \"kl_coeff\": 1.0, \"num_workers\": 4, \"num_cpus\": 1, # number of CPUs to use per trial \"num_gpus\": 0, # number of GPUs to use per trial # For DiagGaussian action distributions, make the second half of the model # outputs floating bias variables instead of state-dependent. This only # has an effect is using the default fully connected net. # avoid this for non-continuous action spaces \"model\": {\"free_log_std\": True}, # These params are tuned from a fixed starting value. \"lambda\": 0.95, \"clip_param\": 0.2, \"lr\": 1e-4, # These params start off randomly drawn from a set. \"num_sgd_iter\": tune.choice([10, 20, 30]), \"sgd_minibatch_size\": tune.choice([128, 512, 2048]), \"train_batch_size\": tune.choice([10000, 20000, 40000]), }, # MLFlow callback uses parent_run_id and tracks all hyperparameter # runs as child jobs run_config=air.RunConfig( stop=stopping_criteria, callbacks=[ MLflowLoggerCallback( tags={MLFLOW_PARENT_RUN_ID: current_run.info.run_id}, experiment_name=\"pbt_ppo\", save_artifact=True, ) ], ), ) Here we are passing the algorithm we want to tune (PPO), the tuning configuration, the initial configuration of the algorithm, and the number of workers to use for sampling.","title":"Instantiating the Tuner"},{"location":"user_guides/hyperparameter-tuning/#param-space-vs-hyperparameter-mutations","text":"In the above Tuner call, we pass the param_space dictionary, which specifies the initial configuration of the algorithm. This is the configuration that is used to initialize the algorithm and is then mutated by the PBT algorithm. If the parameters were ommitted from param_space but were provided in hyperparam_mutations , then the initial values would be also sampled from hyperparam_mutations . The hyperparameters that are specified in the param_space dictionary are not mutated by the PBT algorithm unless they are also in hyperparam_mutations . For example, in the above code snippet, the num_workers hyperparameter is specified in the param_space dictionary but not in hyperparam_mutations , and is therefore not mutated by the PBT algorithm. Conversely, the num_sgd_iter hyperparameter is specified in the hyperparam_mutations dictionary and is mutated by the PBT algorithm.","title":"Param Space vs Hyperparameter Mutations"},{"location":"user_guides/hyperparameter-tuning/#selecting-the-number-of-cpus-and-workers-for-tuning","text":"Since PBT can train multiple trials concurrently, you can speed up the trial runtime by leveraging available CPUs to parallelize training. To do so effectively, you will need to specify the number of CPUs and workers to use for tuning. The num_cpus parameter specifies the number of CPUs to use for each trial, and the num_workers parameter specifies the number of workers to use for sampling from your simulation environments. In the above code snippet, we are using 4 workers and 1 CPU per trial, so we will need 5 CPUs to run each trial. If you do not have enough CPUs to run all the trials concurrently (i.e., 5 * num_tune_samples ), then the PBT scheduler will round-robin between training each subsequent trial. For example, if you only have 10 CPUs (as we have requested in our AML job.yml for this experiment), then the PBT scheduler will train 2 trials concurrently, and then train the remaining trials sequentially as resources become available.","title":"Selecting the Number of CPUs and Workers for Tuning"},{"location":"user_guides/hyperparameter-tuning/#further-resources","text":"Using RLlib with Tune Visualizing PBT Guide to Parallelism and Resources for Ray Tune A Guide to PBT with Tune","title":"Further Resources"},{"location":"user_guides/monitoring-mlflow/","text":"Logging Metrics and Monitoring Experiments with MLFlow Overview The Plato toolkit integrates with MLFlow for monitoring experiments, saving model checkpoints, and logging metrics. This integration allows you to view your model's training progress over time, view and download snapshots of your model over time, and compare multiple runs together. This sample examples/hyperparameter-tuning-and-monitoring shows how to use the MLFlow integration with Plato. This example uses the MLflowLoggerCallback from Ray Tune to log the results to MLflow. The MLflow integration allows you to log all the artifacts produced by Ray Tune, such as the model checkpoints, to MLflow. For more information on available options and customizations, see the documentation for the callback. How Logging is Setup in the Sample The sample uses ray.air.RunConfig for configuring training and tuning runs. This is where we provide the callback for mlflow and ask it to save model checkpoints and metrics. run_config=air.RunConfig( stop=stopping_criteria, callbacks=[ MLflowLoggerCallback( tags={MLFLOW_PARENT_RUN_ID: current_run.info.run_id}, experiment_name=\"pbt_ppo\", save_artifact=True, ) ], ) Here we are setting up a tag to ensure all the subsequent sweeping jobs are run under the same parent job, making it easier to view the results on AzureML. Additionally, we ask MLflow to save all the artifacts produced by Ray Tune, which includes the hyperparameter values and model checkpoints. Viewing Local Runs using the MLFlow UI If you use the --test-local option when running the sample, your results will be saved locally in a folder called mlruns . mlruns \u251c\u2500\u2500 0 \u2502 \u251c\u2500\u2500 9fcea3f4faf845a3be62c60bfdd24f30 \u2502 \u2502 \u251c\u2500\u2500 artifacts \u2502 \u2502 \u251c\u2500\u2500 meta.yaml \u2502 \u2502 \u251c\u2500\u2500 metrics \u2502 \u2502 \u251c\u2500\u2500 params \u2502 \u2502 \u2514\u2500\u2500 tags \u2502 \u2502 \u251c\u2500\u2500 mlflow.source.git.commit \u2502 \u2502 \u251c\u2500\u2500 mlflow.source.name \u2502 \u2502 \u251c\u2500\u2500 mlflow.source.type \u2502 \u2502 \u2514\u2500\u2500 mlflow.user \u2502 \u2514\u2500\u2500 meta.yaml \u2514\u2500\u2500 1 \u251c\u2500\u2500 c0844d850fef427c8d861ad5f4cd7fdd \u2502 \u251c\u2500\u2500 artifacts \u2502 \u2502 \u251c\u2500\u2500 checkpoint_000010 \u2502 \u2502 \u2502 \u251c\u2500\u2500 algorithm_state.pkl \u2502 \u2502 \u2502 \u251c\u2500\u2500 policies \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 default_policy \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 policy_state.pkl \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 rllib_checkpoint.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 rllib_checkpoint.json \u2502 \u2502 \u251c\u2500\u2500 params.json \u2502 \u2502 \u251c\u2500\u2500 params.pkl \u2502 \u2502 \u251c\u2500\u2500 progress.csv \u2502 \u2502 \u2514\u2500\u2500 result.json \u2502 \u251c\u2500\u2500 meta.yaml \u2502 \u251c\u2500\u2500 metrics \u2502 \u2502 \u251c\u2500\u2500 agent_timesteps_total \u2502 \u2502 \u251c\u2500\u2500 done \u2502 \u2502 \u251c\u2500\u2500 episode_len_mean \u2502 \u2502 \u251c\u2500\u2500 episode_reward_max \u2502 \u2502 \u251c\u2500\u2500 episode_reward_mean \u2502 \u2502 \u251c\u2500\u2500 episode_reward_min \u2502 \u2502 \u251c\u2500\u2500 episodes_this_iter \u2502 \u2502 \u251c\u2500\u2500 episodes_total \u2502 \u2502 \u251c\u2500\u2500 iterations_since_restore \u2502 \u2502 \u251c\u2500\u2500 num_agent_steps_sampled \u2502 \u2502 \u251c\u2500\u2500 num_agent_steps_trained \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_sampled \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_sampled_this_iter \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_trained \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_trained_this_iter \u2502 \u2502 \u251c\u2500\u2500 num_faulty_episodes \u2502 \u2502 \u251c\u2500\u2500 num_healthy_workers \u2502 \u2502 \u251c\u2500\u2500 num_in_flight_async_reqs \u2502 \u2502 \u251c\u2500\u2500 num_remote_worker_restarts \u2502 \u2502 \u251c\u2500\u2500 num_steps_trained_this_iter \u2502 \u2502 \u251c\u2500\u2500 pid \u2502 \u2502 \u251c\u2500\u2500 time_since_restore \u2502 \u2502 \u251c\u2500\u2500 time_this_iter_s \u2502 \u2502 \u251c\u2500\u2500 time_total_s \u2502 \u2502 \u251c\u2500\u2500 timestamp \u2502 \u2502 \u251c\u2500\u2500 timesteps_since_restore \u2502 \u2502 \u251c\u2500\u2500 timesteps_total \u2502 \u2502 \u251c\u2500\u2500 training_iteration \u2502 \u2502 \u2514\u2500\u2500 warmup_time \u2502 \u251c\u2500\u2500 params \u2502 \u2502 \u251c\u2500\u2500 clip_param \u2502 \u2502 \u251c\u2500\u2500 env \u2502 \u2502 \u251c\u2500\u2500 kl_coeff \u2502 \u2502 \u251c\u2500\u2500 lambda \u2502 \u2502 \u251c\u2500\u2500 lr \u2502 \u2502 \u251c\u2500\u2500 num_cpus \u2502 \u2502 \u251c\u2500\u2500 num_gpus \u2502 \u2502 \u251c\u2500\u2500 num_sgd_iter \u2502 \u2502 \u251c\u2500\u2500 num_workers \u2502 \u2502 \u251c\u2500\u2500 sgd_minibatch_size \u2502 \u2502 \u2514\u2500\u2500 train_batch_size \u2502 \u2514\u2500\u2500 tags \u2502 \u251c\u2500\u2500 mlflow.parentRunId \u2502 \u251c\u2500\u2500 mlflow.runName \u2502 \u2514\u2500\u2500 trial_name \u2514\u2500\u2500 meta.yaml You can launch a local UI for mlflow by running mlflow ui from the samples directory. Here you can view model checkpoints and a table summary of all the runs. Viewing Remote Runs on AzureML If you instead ran these experiments on AzureML remotely, you should use the AzureML Studio to view your experiments. Viewing Job Runs and Filtering Your Experiment If you navigate to your AML workspace and click on the jobs tab, you should see all the experiments you have run. By default, AML will only show the parent jobs, which does not include the jobs that run the actual hyperparameter sweeps. To view all the jobs, you can click on the \"Include child jobs\" button at the top of the screen. You can then filter based on time to view the most recent runs, or by experiment name to see the ones relevant to your specific experiment. The charts will also be automatically updated based on your filter. Some useful filters you can run: Filter on episode_reward_mean to the highest value or above a certain threshold. This can help you identify the models that perform the best. Filter on agent_timesteps_total to see the models that have been trained for the longest amount of sample steps. Filter on time_total_s to see the models that have been trained for the longest amount of time. Filter on job status to see the jobs that have completed successfully, are still running or have failed. Adding Custom Charts If you want to change the default charts that are displayed for your experiments in AzureML Studio, you can create a custom chart and save it as the default. To do this, you can follow these steps: Open your experiment in AzureML Studio. Click on the \"Charts\" tab to view the default charts. Create a new chart by clicking on the \"New Chart\" button. Configure the chart to display the metric(s) that you want to be the default, such as episode_reward_mean . Once you are satisfied with the chart, click on the \"Save as Default\" button at the bottom of the screen. After you have saved your custom chart as the default, it should be displayed automatically whenever you open your experiment in AzureML Studio. Note that this will only affect your own view of the experiment; other users will still see the original default charts unless they also create and save their own custom charts. Downloading Checkpoints From AML Registry After you have completed your training jobs, you can download the checkpoints from the AML registry to your local machine. This can be useful if you want to run inference on your model locally or deploy it to a different environment. The script get_mlflow_artifacts.py provides some helper functions for downloading the top-performing checkpoint from your AML experiments to your local machine. Specifically, the following code chunk will download the top checkpoint from the most recent experiment to the model_checkpoints directory. If you want to download from a different experiment, you can change the experiment variable to the name of the experiment you want to download from (you can use something like [a.name for a in experiments] to view each experiment by its name) from get_mlflow_artifacts import get_top_run, list_experiments experiments = list_experiments() # pick the experiment you run experiment = experiments[-1].name local_path = \"model_checkpoints\" get_top_run(experiment, local_path, rank_metric=\"metrics.episode_reward_max\") Note the argument rank_metric for the get_top_run function. This argument specifies which metric to use to rank the checkpoints. By default, it is set to metrics.episode_reward_mean , which will rank the checkpoints by the average episode reward. You can change this to any of the other metrics that are logged by the training script, such as metrics.episode_reward_max or metrics.episode_reward_min . You can also change the local_path argument to specify a different directory to download the checkpoints to.","title":"Monitoring and Logging with MLFlow"},{"location":"user_guides/monitoring-mlflow/#logging-metrics-and-monitoring-experiments-with-mlflow","text":"","title":"Logging Metrics and Monitoring Experiments with MLFlow"},{"location":"user_guides/monitoring-mlflow/#overview","text":"The Plato toolkit integrates with MLFlow for monitoring experiments, saving model checkpoints, and logging metrics. This integration allows you to view your model's training progress over time, view and download snapshots of your model over time, and compare multiple runs together. This sample examples/hyperparameter-tuning-and-monitoring shows how to use the MLFlow integration with Plato. This example uses the MLflowLoggerCallback from Ray Tune to log the results to MLflow. The MLflow integration allows you to log all the artifacts produced by Ray Tune, such as the model checkpoints, to MLflow. For more information on available options and customizations, see the documentation for the callback.","title":"Overview"},{"location":"user_guides/monitoring-mlflow/#how-logging-is-setup-in-the-sample","text":"The sample uses ray.air.RunConfig for configuring training and tuning runs. This is where we provide the callback for mlflow and ask it to save model checkpoints and metrics. run_config=air.RunConfig( stop=stopping_criteria, callbacks=[ MLflowLoggerCallback( tags={MLFLOW_PARENT_RUN_ID: current_run.info.run_id}, experiment_name=\"pbt_ppo\", save_artifact=True, ) ], ) Here we are setting up a tag to ensure all the subsequent sweeping jobs are run under the same parent job, making it easier to view the results on AzureML. Additionally, we ask MLflow to save all the artifacts produced by Ray Tune, which includes the hyperparameter values and model checkpoints.","title":"How Logging is Setup in the Sample"},{"location":"user_guides/monitoring-mlflow/#viewing-local-runs-using-the-mlflow-ui","text":"If you use the --test-local option when running the sample, your results will be saved locally in a folder called mlruns . mlruns \u251c\u2500\u2500 0 \u2502 \u251c\u2500\u2500 9fcea3f4faf845a3be62c60bfdd24f30 \u2502 \u2502 \u251c\u2500\u2500 artifacts \u2502 \u2502 \u251c\u2500\u2500 meta.yaml \u2502 \u2502 \u251c\u2500\u2500 metrics \u2502 \u2502 \u251c\u2500\u2500 params \u2502 \u2502 \u2514\u2500\u2500 tags \u2502 \u2502 \u251c\u2500\u2500 mlflow.source.git.commit \u2502 \u2502 \u251c\u2500\u2500 mlflow.source.name \u2502 \u2502 \u251c\u2500\u2500 mlflow.source.type \u2502 \u2502 \u2514\u2500\u2500 mlflow.user \u2502 \u2514\u2500\u2500 meta.yaml \u2514\u2500\u2500 1 \u251c\u2500\u2500 c0844d850fef427c8d861ad5f4cd7fdd \u2502 \u251c\u2500\u2500 artifacts \u2502 \u2502 \u251c\u2500\u2500 checkpoint_000010 \u2502 \u2502 \u2502 \u251c\u2500\u2500 algorithm_state.pkl \u2502 \u2502 \u2502 \u251c\u2500\u2500 policies \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 default_policy \u2502 \u2502 \u2502 \u2502 \u251c\u2500\u2500 policy_state.pkl \u2502 \u2502 \u2502 \u2502 \u2514\u2500\u2500 rllib_checkpoint.json \u2502 \u2502 \u2502 \u2514\u2500\u2500 rllib_checkpoint.json \u2502 \u2502 \u251c\u2500\u2500 params.json \u2502 \u2502 \u251c\u2500\u2500 params.pkl \u2502 \u2502 \u251c\u2500\u2500 progress.csv \u2502 \u2502 \u2514\u2500\u2500 result.json \u2502 \u251c\u2500\u2500 meta.yaml \u2502 \u251c\u2500\u2500 metrics \u2502 \u2502 \u251c\u2500\u2500 agent_timesteps_total \u2502 \u2502 \u251c\u2500\u2500 done \u2502 \u2502 \u251c\u2500\u2500 episode_len_mean \u2502 \u2502 \u251c\u2500\u2500 episode_reward_max \u2502 \u2502 \u251c\u2500\u2500 episode_reward_mean \u2502 \u2502 \u251c\u2500\u2500 episode_reward_min \u2502 \u2502 \u251c\u2500\u2500 episodes_this_iter \u2502 \u2502 \u251c\u2500\u2500 episodes_total \u2502 \u2502 \u251c\u2500\u2500 iterations_since_restore \u2502 \u2502 \u251c\u2500\u2500 num_agent_steps_sampled \u2502 \u2502 \u251c\u2500\u2500 num_agent_steps_trained \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_sampled \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_sampled_this_iter \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_trained \u2502 \u2502 \u251c\u2500\u2500 num_env_steps_trained_this_iter \u2502 \u2502 \u251c\u2500\u2500 num_faulty_episodes \u2502 \u2502 \u251c\u2500\u2500 num_healthy_workers \u2502 \u2502 \u251c\u2500\u2500 num_in_flight_async_reqs \u2502 \u2502 \u251c\u2500\u2500 num_remote_worker_restarts \u2502 \u2502 \u251c\u2500\u2500 num_steps_trained_this_iter \u2502 \u2502 \u251c\u2500\u2500 pid \u2502 \u2502 \u251c\u2500\u2500 time_since_restore \u2502 \u2502 \u251c\u2500\u2500 time_this_iter_s \u2502 \u2502 \u251c\u2500\u2500 time_total_s \u2502 \u2502 \u251c\u2500\u2500 timestamp \u2502 \u2502 \u251c\u2500\u2500 timesteps_since_restore \u2502 \u2502 \u251c\u2500\u2500 timesteps_total \u2502 \u2502 \u251c\u2500\u2500 training_iteration \u2502 \u2502 \u2514\u2500\u2500 warmup_time \u2502 \u251c\u2500\u2500 params \u2502 \u2502 \u251c\u2500\u2500 clip_param \u2502 \u2502 \u251c\u2500\u2500 env \u2502 \u2502 \u251c\u2500\u2500 kl_coeff \u2502 \u2502 \u251c\u2500\u2500 lambda \u2502 \u2502 \u251c\u2500\u2500 lr \u2502 \u2502 \u251c\u2500\u2500 num_cpus \u2502 \u2502 \u251c\u2500\u2500 num_gpus \u2502 \u2502 \u251c\u2500\u2500 num_sgd_iter \u2502 \u2502 \u251c\u2500\u2500 num_workers \u2502 \u2502 \u251c\u2500\u2500 sgd_minibatch_size \u2502 \u2502 \u2514\u2500\u2500 train_batch_size \u2502 \u2514\u2500\u2500 tags \u2502 \u251c\u2500\u2500 mlflow.parentRunId \u2502 \u251c\u2500\u2500 mlflow.runName \u2502 \u2514\u2500\u2500 trial_name \u2514\u2500\u2500 meta.yaml You can launch a local UI for mlflow by running mlflow ui from the samples directory. Here you can view model checkpoints and a table summary of all the runs.","title":"Viewing Local Runs using the MLFlow UI"},{"location":"user_guides/monitoring-mlflow/#viewing-remote-runs-on-azureml","text":"If you instead ran these experiments on AzureML remotely, you should use the AzureML Studio to view your experiments.","title":"Viewing Remote Runs on AzureML"},{"location":"user_guides/monitoring-mlflow/#viewing-job-runs-and-filtering-your-experiment","text":"If you navigate to your AML workspace and click on the jobs tab, you should see all the experiments you have run. By default, AML will only show the parent jobs, which does not include the jobs that run the actual hyperparameter sweeps. To view all the jobs, you can click on the \"Include child jobs\" button at the top of the screen. You can then filter based on time to view the most recent runs, or by experiment name to see the ones relevant to your specific experiment. The charts will also be automatically updated based on your filter. Some useful filters you can run: Filter on episode_reward_mean to the highest value or above a certain threshold. This can help you identify the models that perform the best. Filter on agent_timesteps_total to see the models that have been trained for the longest amount of sample steps. Filter on time_total_s to see the models that have been trained for the longest amount of time. Filter on job status to see the jobs that have completed successfully, are still running or have failed.","title":"Viewing Job Runs and Filtering Your Experiment"},{"location":"user_guides/monitoring-mlflow/#adding-custom-charts","text":"If you want to change the default charts that are displayed for your experiments in AzureML Studio, you can create a custom chart and save it as the default. To do this, you can follow these steps: Open your experiment in AzureML Studio. Click on the \"Charts\" tab to view the default charts. Create a new chart by clicking on the \"New Chart\" button. Configure the chart to display the metric(s) that you want to be the default, such as episode_reward_mean . Once you are satisfied with the chart, click on the \"Save as Default\" button at the bottom of the screen. After you have saved your custom chart as the default, it should be displayed automatically whenever you open your experiment in AzureML Studio. Note that this will only affect your own view of the experiment; other users will still see the original default charts unless they also create and save their own custom charts.","title":"Adding Custom Charts"},{"location":"user_guides/monitoring-mlflow/#downloading-checkpoints-from-aml-registry","text":"After you have completed your training jobs, you can download the checkpoints from the AML registry to your local machine. This can be useful if you want to run inference on your model locally or deploy it to a different environment. The script get_mlflow_artifacts.py provides some helper functions for downloading the top-performing checkpoint from your AML experiments to your local machine. Specifically, the following code chunk will download the top checkpoint from the most recent experiment to the model_checkpoints directory. If you want to download from a different experiment, you can change the experiment variable to the name of the experiment you want to download from (you can use something like [a.name for a in experiments] to view each experiment by its name) from get_mlflow_artifacts import get_top_run, list_experiments experiments = list_experiments() # pick the experiment you run experiment = experiments[-1].name local_path = \"model_checkpoints\" get_top_run(experiment, local_path, rank_metric=\"metrics.episode_reward_max\") Note the argument rank_metric for the get_top_run function. This argument specifies which metric to use to rank the checkpoints. By default, it is set to metrics.episode_reward_mean , which will rank the checkpoints by the average episode reward. You can change this to any of the other metrics that are logged by the training script, such as metrics.episode_reward_max or metrics.episode_reward_min . You can also change the local_path argument to specify a different directory to download the checkpoints to.","title":"Downloading Checkpoints From AML Registry"}]}